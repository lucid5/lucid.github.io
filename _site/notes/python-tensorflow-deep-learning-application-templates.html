<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Should we define batch size during graph construction?</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@" /><meta name="twitter:title" content="Should we define batch size during graph construction?" /><meta name="twitter:description" content="Intro"><meta name="description" content="Intro"><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/python-tensorflow-deep-learning-application-templates"><link rel="alternate" type="application/atom+xml" title="thtrieu" href="/feed.xml" /></head><body><aside class="logo"> <a href="/"> <img src="http://i.imgur.com/tU08Pu4.gif" id = "logo" onmouseover = "thatLogo()" onmouseout = "thisLogo()" onmouseup = "clicked()" class= "gravatar"> </a></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1 onmouseover="buttonLogo()" onmouseout = "thisLogo()"><a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#116;&#104;&#116;&#114;&#105;&#101;&#117;&#064;&#097;&#112;&#099;&#115;&#046;&#118;&#110;" target = "_blank"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a>     <a href="http://github.com/thtrieu/" target = "_blank"><img src="http://i.imgur.com/aV59QS6.png (github icon with padding)" alt="alt text" /></a>     <a href="https://linkedin.com/in/trinhhtrieu" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="http://thtrieu.github.io/resume.pdf" target = "_blank"><img src="http://i.imgur.com/2amdaUm.png (resume icon with padding)" alt="alt text" /></a>     <a href="#share"><img src="http://i.imgur.com/GE4GmC3.png (share icon with padding)" alt="alt text" /></a></h1><time>March 7, 2016 - just another day in heaven</time></div><!--<div class="divider"></div>--><p>&nbsp;</p><p></p><h3 id="intro">Intro</h3><p>So I’ve seen a number of Deep Learning snippets using <code class="highlighter-rouge">Tensorflow</code>, and reused a mixture of them here and there in my code. But there are things that cannot be mixed. Some codes follow their own philosophy (okay let’s call it <strong>taste</strong>) and refuse incorporating some certain kind of code into its workflow (It is possible that this is because of my noop technical ability, but nevertheless there are points worth mentioning)</p><p>Studying this separation can be really beneficial for starting out with <code class="highlighter-rouge">Tensorflow</code>. Here I present two templates that I would comfortably switch back and forth between them depending on the deployed model. (And ofcourse they cannot be mixed)</p><h3 id="review">Review</h3><p>Again I would throw a terrible generalised statement here about the workflow of any Deep Learning experiment:</p><ol><li><em>You read data (supervised one - features \( X \) and label \( Y\) )</em></li><li><em>If you don’t have any test set, then use 10-fold Cross Validation</em></li><li><em>Now that you have train and test sets, hold-out a part of the train set for early stopping, if the size of train set is big enough</em></li><li><em>Now go on training (in batches), while simultaenously looking at the hold-out set to decide when to stop. Then stop and evaluate the model on test set, report it (if you are using 10 fold CV, you’ll have to average the numbers)</em>.</li></ol><p>So, this is the first code template of the two, it will look something like this:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">data_manipulator</span> <span class="c1"># you build this yourself
</span>
<span class="c1"># Set up data
</span><span class="n">hope</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">hyperParameters</span><span class="o">.</span><span class="n">Hold_Out_PErcentage</span> 
<span class="n">train</span><span class="p">,</span> <span class="n">hold_out</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">data_manipulator</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">input_folder</span><span class="p">,</span> <span class="n">hope</span><span class="p">)</span>

<span class="c1"># Set up model
</span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperParameters</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Set_up_layers</span><span class="p">(</span><span class="n">hyperParameters</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">Set_up_layers</span><span class="p">(</span><span class="n">hyperParameters</span><span class="p">):</span>
        <span class="c1"># 1. Set up placeholders self.x and self.y
</span>        <span class="c1"># 2. build layers gradually upon self.x 
</span>        <span class="c1">#    all the way through to the output layer
</span>        <span class="c1"># 3. calculate loss and accuracy, using self.y
</span>        <span class="k">return</span><span class="p">((</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>

<span class="c1"># Session switch on
</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="nb">float</span><span class="p">()</span> <span class="c1"># report this after the experiment
</span>
<span class="k">with</span> <span class="n">sess</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    
    <span class="c1"># Set up the graph
</span>    <span class="n">hyperParams</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">hyperParameters</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hyperParams</span><span class="p">)</span>

	<span class="c1"># Set up trainer
</span>	<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span> <span class="c1"># Adagrad, RMSprop, etc.
</span>	<span class="n">gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">loss_op</span><span class="p">)</span>
	<span class="n">train_op</span>  <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>

    <span class="c1"># Post-training set-ups
</span>    <span class="n">max_epoch</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="o">.</span><span class="n">max_epoch</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="n">max_hold_acc</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="c1"># keep track of the accuracy on hold-out set
</span>    
    <span class="c1"># Now start the training
</span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_manipulator</span><span class="o">.</span><span class="n">yield_batch</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">):</span>
        <span class="c1"># 1. Let the tensors flows
</span>        <span class="c1">#    with gradients calculated and variables updated
</span>        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
        
        <span class="c1"># 2a. Let the tensors simply flows through
</span>        <span class="c1">#     you may want to do this step much less frequently
</span>        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">hold_out</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">hold_out</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
        <span class="n">hold_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
        
        <span class="c1"># 2b. hold out peaked, report accuracy on test
</span>        <span class="k">if</span> <span class="n">hold_acc</span> <span class="o">&gt;=</span> <span class="n">max_hold_acc</span><span class="p">:</span>
            <span class="n">max_hold_acc</span> <span class="o">=</span> <span class="n">hold_acc</span>
            <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
    
<span class="c1"># now publish your paper maybe?
</span><span class="k">print</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
</code></pre></div></div><p>The second template comes later on in this post, which I initially consider being completely lack of good taste, but it solves problems. Go on reading if you are curious :D</p><h3 id="okay-so-where-is-the-funinterestingtroublesome-part">Okay, So where is the fun/interesting/troublesome part?</h3><h4 id="the-troublesome-batch_size-placeholder">The troublesome <code class="highlighter-rouge">batch_size</code> placeholder</h4><p>The hidden thing in the above code snippet is that, there are other placeholders besides <code class="highlighter-rouge">m.x</code> and <code class="highlighter-rouge">m.y</code> to be fed into <code class="highlighter-rouge">feed_dict</code>. Most likely they are <code class="highlighter-rouge">m.drop_out_probability</code> (yeah, very powerful regularisation technique - definitely recommended in every deep model) and <code class="highlighter-rouge">m.batch_size</code>. Why are they being placeholders? Because in the evaluation steps (<code class="highlighter-rouge">2a</code> and <code class="highlighter-rouge">2b</code>), they have a different value to step <code class="highlighter-rouge">1</code>. Namely, <code class="highlighter-rouge">model.drop_out_probability</code> must be \( 1.0 \) in both <code class="highlighter-rouge">2a</code> and <code class="highlighter-rouge">2b</code>, while <code class="highlighter-rouge">model.batch_size</code> must be <code class="highlighter-rouge">hold_out.size</code> in <code class="highlighter-rouge">2a</code> and <code class="highlighter-rouge">test.size</code> in <code class="highlighter-rouge">2b</code>.</p><p>The good <strong>taste</strong> here is that: the viewpoint above about <code class="highlighter-rouge">batch_size</code> generalizes very well from training set to both hold-out set and test set. It is brilliant to view hold-out and test sets to be two big fat batches, because who evaluates models <em>in small batches</em> anyway (slower execution for the same output)? (well, I can imagine people do this in specific situations, like when hold-out and test sets are too big to fit in memory, but this is rare for my daily experiments). When there is a generalization, there is less branching statements in your code, and when the code is cleaner, people will just love it.</p><p>But this also means, the actual value of <code class="highlighter-rouge">hyperParameters.batch_size</code>, although being passed to the initiation of <code class="highlighter-rouge">m</code>, will not be used because it is not applicable for hold-out and test sets. But hold on, you are <em>fine</em> not specifying <code class="highlighter-rouge">model.batch_size</code> as the placeholder in almost many cases, because <code class="highlighter-rouge">Tensorflow</code> allows tensors with <code class="highlighter-rouge">None</code> dimension, as long as at run-time, tensor operations work out fine (e.g. tensor multiplication requires some dimension-matching) (cool!). That being said, there are cases when you do have to specifically refer to <code class="highlighter-rouge">batch_size</code> in the constructure phase (a.k.a. initiating <code class="highlighter-rouge">Model</code> object). For an example, the code below is from tensorflow’s github repository:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="s">"""Return state tensor (shape [batch_size x state_size]) filled with 0.
    Args:
      batch_size: int, float, or unit Tensor representing the batch size.
      dtype: the data type to use for the state.
    Returns:
      A 2D Tensor of shape [batch_size x state_size] filled with zeros.
    """</span>
</code></pre></div></div><p><code class="highlighter-rouge">zero_state</code> is a function that most will use in every Recurrent model, see the documentation? <code class="highlighter-rouge">batch_size</code> should be <code class="highlighter-rouge">int</code>, <code class="highlighter-rouge">float</code> if you know the actual value at construction phase, or you can throw in a placeholder if you don’t know its value.</p><h4 id="so-what-is-the-problem-anyway">So what is the problem anyway?</h4><p>The problem is, there are cases you <em>desparately</em> have the need to iterate over <code class="highlighter-rouge">range(0,batch_size)</code> at construction phase. But when <code class="highlighter-rouge">batch_size</code> being a placeholder (you don’t know the value at construction phase), iterate over it makes no sense. Yes, <code class="highlighter-rouge">Python</code> raise an error immediately when I desperately (and naively) try this loop expecting some magical colaboration between <code class="highlighter-rouge">range()</code> and <code class="highlighter-rouge">Tensorflow</code>’s placeholders.</p><p>Is this particular looping need <em>frequent and universal</em>? I’ll say yes, as I and my colleagues encountered it for quite a number of times. In <a href="http://thtrieu.github.io/2016/02/15/python-tensorflow-catching-the-last-output">this post</a> I point out a concrete situation of this looping need and a (cool) walk-around for it. Notice I call it a <em>walk around</em>. Because it is not a solution, I don’t know if there is any built-in <code class="highlighter-rouge">Tensorflow</code> op, like <code class="highlighter-rouge">tensorflow.range()</code> or something, for people to loop over placeholders at construction phase. But the fact is, you <strong>DON’T</strong> need such operation at all, as long as you use the following template (which I personally considered to be terribly lack of <strong>taste</strong>, but nevertheless works wonders)</p><p>Here it comes,</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">data_manipulator</span> <span class="c1"># you build this yourself
</span>
<span class="c1"># Set up data
</span><span class="n">hope</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">hyperParameters</span><span class="o">.</span><span class="n">Hold_Out_PErcentage</span> 
<span class="n">train</span><span class="p">,</span> <span class="n">hold_out</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">data_manipulator</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">input_folder</span><span class="p">,</span> <span class="n">hope</span><span class="p">)</span>

<span class="c1"># Set up model
</span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperParameters</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Here comes the branching ...
</span>        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">hyperParameters</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Set_up_layers</span><span class="p">(</span><span class="n">hyperParameters</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">Set_up_layers</span><span class="p">(</span><span class="n">hyperParameters</span><span class="p">):</span>
        <span class="c1"># 1. Set up placeholders self.x and self.y
</span>        <span class="c1"># 2. build layers gradually upon self.x 
</span>        <span class="c1">#    all the way through to the output layer,
</span>        <span class="c1">#    feel free to loop over self.batch_size
</span>        <span class="c1">#    it is an actual int/float now!
</span>        <span class="c1"># 3. calculate loss and accuracy, using self.y
</span>        <span class="k">return</span><span class="p">((</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>

<span class="c1"># Session switch on
</span><span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">test_acc</span> <span class="o">=</span> <span class="nb">float</span><span class="p">()</span> <span class="c1"># report this after the experiment
</span>
<span class="k">with</span> <span class="n">sess</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>

    <span class="c1"># Set up THREE graphs
</span>    <span class="n">hyperParams</span> <span class="o">=</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">hyperParameters</span>
    <span class="n">m_train</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hyperParams</span><span class="p">)</span>
    <span class="n">m_hold</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hyperParams</span><span class="p">,</span> <span class="n">hold_out</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="n">m_test</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">hyperParams</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

    <span class="c1"># Set up trainer
</span>	<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span> <span class="c1"># Adagrad, RMSprop, etc.
</span>	<span class="n">gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">loss_op</span><span class="p">)</span>
	<span class="n">train_op</span>  <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
    
    <span class="c1"># Post-training set-ups
</span>    <span class="n">max_epoch</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="o">.</span><span class="n">max_epoch</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hyperParams</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="n">max_hold_acc</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span> <span class="c1"># keep track of the accuracy on hold-out set
</span>    
    <span class="c1"># Now start training
</span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_manipulator</span><span class="o">.</span><span class="n">yield_batch</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_epoch</span><span class="p">):</span>
        <span class="c1"># 1. Let the tensors flows
</span>        <span class="c1">#    with gradients calculated and variables updated
</span>        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m_train</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m_train</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">m_train</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
        
        <span class="c1"># 2a. Let the tensors simply flows through
</span>        <span class="c1">#     you may want to do this step much less frequently
</span>        <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m_hold</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">hold_out</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m_hold</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">hold_out</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
        <span class="n">hold_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">m_hold</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
        
        <span class="c1"># 2b. hold out peaked, report accuracy on test
</span>        <span class="k">if</span> <span class="n">hold_acc</span> <span class="o">&gt;=</span> <span class="n">max_hold_acc</span><span class="p">:</span>
            <span class="n">max_hold_acc</span> <span class="o">=</span> <span class="n">hold_acc</span>
            <span class="n">feed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">m_test</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">m_test</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">y</span><span class="p">}</span>
            <span class="n">test_acc</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">m_test</span><span class="o">.</span><span class="n">accuracy</span><span class="p">],</span> <span class="n">feed_dict</span><span class="p">)</span>
    
<span class="c1"># now publish your paper maybe?
</span><span class="k">print</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
</code></pre></div></div><p>Now see that this template has an <code class="highlighter-rouge">if</code> branching, thus it is kinda more verbose (uglier) than the previous one, not to mention it is longer and obviously requires more memory. Yes, there are ways to avoid the <code class="highlighter-rouge">if</code> but you will not be able to avoid instantiating <em>three different</em> objects of class <code class="highlighter-rouge">Model</code>. The ugly part of this is that, these three objects, although containing only two <code class="highlighter-rouge">Tensorflow</code>-op attributes each, correspond to three separate underlying <code class="highlighter-rouge">Tensorflow</code> graphs. These graphs can be huge if your model is big enough, and guess what? All three of them are <strong>identical</strong> except for the loop part!</p><p>For me, the redundancy here is unbearable at the time discovering this template. But over time, it gradually becomes acceptable because <em>three</em> is a constant anyway. And constants are fine, look, they are \( O(1) \). Nevertheless, whenever I encounter a loop need, the first thing I would do is to stick with <strong>template 1</strong> until I cannot find a walk-around for it. Because walk-arounds are conceptually very cool, and every developers need that little stubborness when it comes to coding styles, isn’t it? No? okay then =(.</p><p>&nbsp;</p><p></p><div class = "divider"></div><div class = "center-foot"><h1 id = "share"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="https://twitter.com/intent/tweet/?text=Should we define batch size during graph construction?&url=https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates" target = "_blank"><img src="http://i.imgur.com/phnE5v0.png (twitter icon with padding)" alt="alt text" /></a>     <a href="https://plus.google.com/share?url=https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates" target = "_blank"><img src="http://i.imgur.com/PeridnP.png (google plus icon with padding)" alt="alt text" /></a>     <a href="https://facebook.com/sharer/sharer.php?u=https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates" target = "_blank"><img src="http://i.imgur.com/GRvQu92.png (facebook icon with padding)" alt="alt text" /></a>     <a href="mailto:?subject=Should we define batch size during graph construction?&body=https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a></h1></div><aside class="back"> <a href="/notes/python-tensorflow-catching-the-last-output">&laquo; Catching the last output of RNN while training in batches</a></aside><aside class="next"> <a href="/notes/r-environment-and-scope-modify-list-of-funcs">Scopes, environments, locked bindings, modify list of functions &raquo;</a></aside><p>&nbsp;</p><p></p><div id="disqus_thread"></div><script> /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates'; this.page.identifier = 'https://thtrieu.github.io/notes/python-tensorflow-deep-learning-application-templates'; }; (function() { var d = document, s = d.createElement('script'); s.src = '//thtrieu.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); }) (); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article></main></body><script> var that = 'http://i.imgur.com/tU08Pu4.gif'; var thiS = 'http://i.imgur.com/tU08Pu4.gif'; var sub = 'http://i.imgur.com/w6NOJp2.gif'; var but = 'http://i.imgur.com/w6NOJp2.gif'; function postLogo() { document.getElementById("logo").src = 'http://i.imgur.com/tU08Pu4.gif'; } function thatLogo() { document.getElementById("logo").src = that; } function thisLogo() { document.getElementById("logo").src = thiS; } function subLogo() { document.getElementById("logo").src = sub; } function buttonLogo() { document.getElementById("logo").src = but; } function postClicked() { thiS = 'http://i.imgur.com/tU08Pu4.gif'; } function clicked() { thiS = that; } </script></html>