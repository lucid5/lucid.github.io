<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Linear Support Vector Machine on top of a deep net</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@" /><meta name="twitter:title" content="Linear Support Vector Machine on top of a deep net" /><meta name="twitter:description" content=""><meta name="description" content=""><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/engineering-the-last-layer"><link rel="alternate" type="application/atom+xml" title="thtrieu" href="/feed.xml" /></head><body><aside class="logo"> <a href="/"> <img src="http://i.imgur.com/tU08Pu4.gif" id = "logo" onmouseover = "thatLogo()" onmouseout = "thisLogo()" onmouseup = "clicked()" class= "gravatar"> </a></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1 onmouseover="buttonLogo()" onmouseout = "thisLogo()"><a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#116;&#104;&#116;&#114;&#105;&#101;&#117;&#064;&#097;&#112;&#099;&#115;&#046;&#118;&#110;" target = "_blank"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a>     <a href="http://github.com/thtrieu/" target = "_blank"><img src="http://i.imgur.com/aV59QS6.png (github icon with padding)" alt="alt text" /></a>     <a href="https://linkedin.com/in/trinhhtrieu" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="http://thtrieu.github.io/resume.pdf" target = "_blank"><img src="http://i.imgur.com/2amdaUm.png (resume icon with padding)" alt="alt text" /></a>     <a href="#share"><img src="http://i.imgur.com/GE4GmC3.png (share icon with padding)" alt="alt text" /></a></h1><time>September 9, 2016 - just another day in heaven</time></div><!--<div class="divider"></div>--><p>&nbsp;</p><p></p><ul id="markdown-toc"><li><a href="#intro" id="markdown-toc-intro">Intro</a></li><li><a href="#problem-formulation" id="markdown-toc-problem-formulation">Problem formulation</a></li><li><a href="#a-first-attempt-at-constructing-deepsvm-loss" id="markdown-toc-a-first-attempt-at-constructing-deepsvm-loss">A first attempt at constructing deepSVM Loss</a></li><li><a href="#tackle-the-first-constraint-g_1n" id="markdown-toc-tackle-the-first-constraint-g_1n">Tackle the first constraint <script type="math/tex">g_{1n}</script></a></li><li><a href="#second-constraint-g_2n" id="markdown-toc-second-constraint-g_2n">Second constraint <script type="math/tex">g_{2n}</script>:</a></li><li><a href="#deriving-sufficient-conditions-for-equivalence-the-first-case" id="markdown-toc-deriving-sufficient-conditions-for-equivalence-the-first-case">Deriving sufficient conditions for equivalence: The first case</a></li><li><a href="#the-second-case" id="markdown-toc-the-second-case">The second case:</a></li></ul><h3 id="intro">Intro</h3><p>For classification tasks, our deep net can be seen as having two parts: a feature extractor and linear classifier. Innovations usually comes from engineering the former - there is much more room to do so. It is therefore natural to wonder what if we rewire the later instead? A first experiment to do is replacing the usual softmax with some other well-known linear classifiers. Namely in this post I explore the soft-margined Support Vector Machine option, trained in One-Vs-Rest fashion.</p><p>Notice that since the classifier and feature extractor is co-trained, replacing the classifer will change the kind of features learnt by the feature extractor. So the impact is network-wide and thus, do worth some time to consider.</p><p>The rest of this post refreshes the mathematical foundation of SVM. I then proceed to investigate if this formulation is compatible with deep feature extractor.</p><h3 id="problem-formulation">Problem formulation</h3><p>Say you had this deep feature extractor <script type="math/tex">\phi(s_n; \alpha)</script> and would like to plug onto it the final layer being an SVM. <script type="math/tex">\phi</script> in this case is explicit and there is no way of computing <script type="math/tex">\kappa_{mn}</script> without computing <script type="math/tex">\phi_n</script> and <script type="math/tex">\phi_m</script> first. There is no kernel trick this time and the only thing we want to do here is defining a loss that <strong>1)</strong> allow gradients flowing backwards and <strong>2)</strong> faithfully follow the definition of SVM. From the previous part, we know that amounts to the following problem:</p><script type="math/tex; mode=display">\min_{x} \ f(x) \ \ s.t. \ \ g_{1n}(x) \leq 0 \ \ and \ \ g_{2n}(x) \leq 0 \ \forall n</script> <center> where $$x = ( w, b, \xi, \alpha )$$ $$ f(x) = C \sum_{n=1}^{N}\xi_n + \frac{1}{2} {\left \| w \right \|}^2$$ $$g_{1n}(x) = -\xi_n$$ $$g_{2n}(x) = 1 - \xi_n - t_ny(s_n; w, \alpha)$$ $$y(s_n; w, b, \alpha) = w^T\phi(s_n; \alpha) + b$$</center><h3 id="a-first-attempt-at-constructing-deepsvm-loss">A first attempt at constructing deepSVM Loss</h3><p>We can see that the construction of <script type="math/tex">f</script> upon <script type="math/tex">\phi</script> involves differentiable steps, thus back-propagation can comfortably flow the gradient all the way back from <script type="math/tex">f</script> to <script type="math/tex">\phi</script>. It is, however, unclear how to incorporate the constraints into this optimization procedure. The general consensus, I found out, is to simply forget about the constraints and add new penalty terms into the objective to discourage violation. That means, for multi-class hard-margin SVM, the objective is as followed:</p><script type="math/tex; mode=display">L(w, b) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + C \sum_n^N \varphi(1 - t_{kn}y(s_n; W_k, B_k, \alpha))</script><p>Where <script type="math/tex">\varphi(x) = max(0,x)</script>, <script type="math/tex">K</script> is the number of class and <script type="math/tex">W_k</script> and <script type="math/tex">b_k</script> is the corresponding <script type="math/tex">(w, b)</script> of decision surface number <script type="math/tex">k</script> - remember we are optimizing a One-vs-All SVM. The loss function for soft-margin is constructed in a similar fashion</p><script type="math/tex; mode=display">L(x) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N C \xi_{kn}+ \lambda \varphi(-\xi_{kn}) + \beta \varphi(1 - t_{kn}y(s_n; W_k, B_k, \alpha) - \xi_{kn})</script><h3 id="tackle-the-first-constraint-g_1n">Tackle the first constraint <script type="math/tex">g_{1n}</script></h3><p>Apparently the above objectives are not mathematically equivalent to the original one, so plugging such a thing on top of <script type="math/tex">\phi</script> is not exactly doing SVM. Here we try to refine the soft-margin objective presented above to get it as close to the original as possible. Observe that the first constraint simply establish a domain restriction on <script type="math/tex">\xi</script>, but we know that auto differentiation will produce gradients that push <script type="math/tex">\xi</script> below <script type="math/tex">0</script> whenever needed. So an obvious thing to do is to clip the gradient whenever it makes <script type="math/tex">\xi</script> falls below zero.</p><p>But there is a clever mathematical trick to this that accomplishes both: a function that allows optimization over whole-axis domain <em>and</em> produce stationary gradient at zero, thus get rid of the need of gradient clipping <em>as long as</em> the variable is initialised to be non-negative. This is the function <script type="math/tex">x^2</script>, whose derivative <script type="math/tex">2x</script> becomes zero if and only if <script type="math/tex">x^2 = 0</script>. By replacing <script type="math/tex">\xi = \nu^2</script>, we are now able to optimise over <script type="math/tex">\nu</script> freely and obtain a better approximation to our original Loss function, i.e. get clear of the <script type="math/tex">g_{1n}</script> penalty:</p><script type="math/tex; mode=display">L_1(x) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N C \nu_{kn}^2 + \lambda \varphi(1 - t_{kn}y(s_n; W_k, B_k, \alpha) - \nu_{kn}^2)</script><h3 id="second-constraint-g_2n">Second constraint <script type="math/tex">g_{2n}</script>:</h3><p>At this point, it is useful to look at what we are trying to approximate:</p><script type="math/tex; mode=display">\min_x f(x) \ s.t. \ \ g_{kn}(x) \leq 0</script> <center> where $$x = (W, B, \nu, \alpha)$$ $$f(x) =\sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N C \nu_{kn}^2$$ $$g_{kn}(x) = 1 - t_{kn}y(s_n; W_k, B_k, \alpha) - \nu_{kn}^2$$ </center><p>Going in the same direction as vanilla SVM to produce the Primal problem, we know that the following Lagrangian is equivalent to the original one:</p><script type="math/tex; mode=display">L_2(x) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N C \nu_{kn}^2 + \max_{\lambda \geq 0} \lambda_{kn} (1 - t_{kn}y(s_n; W_k, B_k, \alpha) - \nu_{kn}^2)</script><p>Realise that <script type="math/tex">L_2</script> is strikingly similar to <script type="math/tex">L_1</script>. In fact, the similarity is not only syntactic, but also semantic. Suppose <script type="math/tex">0 > g_{ kn } (x)</script>, then <script type="math/tex">\lambda_{kn} g_{kn}(x)</script> can not be positive, setting <script type="math/tex">\lambda_{kn}</script> to zero is the only way to obtain maximization in <script type="math/tex">L_2</script>. For <script type="math/tex">g_{kn}(x) = 0</script> it is clear that there is no room for optimization over <script type="math/tex">\lambda</script> since <script type="math/tex">\lambda_{kn}g_{kn}(x)</script> is always <script type="math/tex">0</script>. So either way, <script type="math/tex">\max_{\lambda \geq 0} \lambda_{kn} g_{kn}(x)</script> <script type="math/tex">=</script> <script type="math/tex">\varphi(g_{kn}(x))</script> <script type="math/tex">=</script> <script type="math/tex">0</script>.</p><p>This gives the incentive to prove that <script type="math/tex">L_1</script> is also similar to <script type="math/tex">L_2</script> in its behavior: a non-constrained objective that achieve the same result to the original one. In fact, we have already done one of the two checks: Whenever <script type="math/tex">x</script> satisfies <strong>all</strong> constraints, minimizing <script type="math/tex">L_1</script> is the same as minimizing <script type="math/tex">f(x)</script> since <script type="math/tex">\varphi(g_{kn}(x)) = 0 \ \forall k, n</script>. This left us with the other, more difficult check: <script type="math/tex">x</script> that <strong>some</strong> <script type="math/tex">g_{kn}(x) > 0</script>. We want this to work in a similar fashion, i.e. to prove that <script type="math/tex">L_1(x)</script> cannot be a local minimum. As a reminder, the primal objective can be summarised as:</p><ul><li><p><script type="math/tex">P(x) = f(x)</script> where <script type="math/tex">x</script> feasible.</p></li><li><p><script type="math/tex">P(x) = + \infty</script> elsewhere, clearly not a minimum.</p></li></ul><p>While so far,</p><ul><li><p><script type="math/tex">L_1(x) = f(x)</script> where <script type="math/tex">x</script> feasible.</p></li><li><p><script type="math/tex">L_1(x)</script> finite elsewhere, but hopefully not a local minimum.</p></li></ul><h3 id="deriving-sufficient-conditions-for-equivalence-the-first-case">Deriving sufficient conditions for equivalence: The first case</h3><p>Again, we look at the simpler case: Suppose <script type="math/tex">x</script> violates <strong>all</strong> constraints. Now have a look at <script type="math/tex">L_1</script>, where <script type="math/tex">\varphi</script> becomes identity:</p><script type="math/tex; mode=display">L_1(x) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N C \nu_{kn}^2 + \lambda (1 - t_{kn}y(s_n; W_k, B_k, \alpha) - \nu_{kn}^2)</script><p>Expand and get rid of the constants, we obtain an equivalent objective:</p><script type="math/tex; mode=display">L_3(x) = \sum_k^K \frac{1}{2} \left \| W_k \right \|^2 + \sum_n^N (C - \lambda) \nu_{kn}^2 - \lambda t_{kn}y(s_n; W_k, B_k, \alpha)</script><p>Taking the derivative with respect to <script type="math/tex">(W_k, \nu_{kn}, B_k)</script> and set them to zero to see the necessary conditions of local minima. Assuming <script type="math/tex">\lambda</script> is a non zero constant, we obtain:</p><script type="math/tex; mode=display">W_k = \lambda \sum_n^Nt_{kn}\phi(s_n; \alpha) \ \forall k \ \ (7)</script> <script type="math/tex; mode=display">\sum_n^N t_{kn} = 0 \ \forall k \ \ (8)</script> <script type="math/tex; mode=display">(C - \lambda)\nu_{kn} = 0 \ \forall k, n \ \ (9)</script><p>From this point, you will quickly see that arguments supporting <script type="math/tex">L_3</script> overpower its opponents.</p><p>Let’s consider cases where at least one of the above conditions is immediately wrong. <script type="math/tex">(8)</script> is clearly not true if <script type="math/tex">K > 2</script> since by definition, the construction of <script type="math/tex">t_{kn}</script> requires <script type="math/tex">\sum_k^K\sum_n^Nt_{kn} = N(2 - K)</script>, which cannot be zero as <script type="math/tex">(8)</script> suggests if <script type="math/tex">K > 2</script>. To prove this, consider the aggregated matrix <script type="math/tex">T</script> with <script type="math/tex">t_{kn}</script> entries: it has a single <script type="math/tex">1</script> entry for each column and <script type="math/tex">-1</script> for all the remainings, and the summation is taking over all of its entries, thus the equality.</p><p>Some will close the case at this point and conclude victory for <script type="math/tex">L_1</script>, but let’s just assume <script type="math/tex">K = 2</script> to see how far we can go. Note that <script type="math/tex">K = 2</script> does not means <script type="math/tex">(8)</script> automatically becomes true, this would require another assumption: an equal number of positive and negative training data points. Assume this assumption is satisfied, it can easily be broken by augmenting data points to produce unbalanced size of training examples, or simply make sure that <strong>N is odd</strong>.</p><h3 id="the-second-case">The second case:</h3><p>This is where things get messy. For some of the K classifiers in our One-vs-All scheme, there exists some data points that violates the second constraint, i.e. <script type="math/tex">\exists k^{'}, n^{'}</script> such that</p><script type="math/tex; mode=display">1 - \nu_{k^{'}n^{'}}^2 > t_{k^{'}n^{'}} ( W_{k^{'}}^T \phi (s_{n^{'}}; \alpha ) + B_{k^{'}} )</script><p>Since the K classifiers are independent and the loss is simply a sum of their losses, we can consider separately one of them and get rid of the subscript <script type="math/tex">k^{'}</script> for simplicity of notation. Let <script type="math/tex">V</script> be the set of data points that violate the above constraint with respect to this specific classifier, then our objective becomes</p><script type="math/tex; mode=display">\frac{1}{2} \left \| W \right \|^2 + \sum_{n = 1}^NC_n\nu_n^2 - \lambda W^T \left ( \sum_{n^{'} \in V} t_{n^{'}} \phi_{n^{'}} \right ) - \lambda B \sum_{n^{'} \in V} t_{n^{'}}</script><p>Where <script type="math/tex">C_n = C</script> if <script type="math/tex">n \not \in V</script> and <script type="math/tex">C_n = C - \lambda</script> if otherwise. Compute the derivatives and set them to zero and we obtain the necessary conditions:</p><script type="math/tex; mode=display">W = \lambda \sum_{n^{'} \in V} t_{n^{'}}\phi_{n^{'}}</script> <script type="math/tex; mode=display">\sum_{n^{'} \in V} t_{n^{'}} = 0</script> <script type="math/tex; mode=display">C_{n^{'}}\nu_{n^{'}} = 0 \ \forall n^{'} \in V</script><p>We generally set <script type="math/tex">C \neq \lambda</script>, so the third condition essentially means if the current parameters are stationary, then all <script type="math/tex">\nu</script> must be zero. By definition, for any data point <script type="math/tex">m^{'} \in V</script>, the following inequality holds</p><script type="math/tex; mode=display">1 - \nu_{m^{'}}^2 = 1 > t_{m^{'}} (W^T\phi_{m^{'}} + B)</script><p>Subtitute <script type="math/tex">W</script> by the first condition and we get:</p><script type="math/tex; mode=display">1 > \lambda t_{m^{'}} \left( \sum_{n^{'} \in V} t_{n^{'}}\phi_{n^{'}}^T\phi_{m^{'}} + B \right)</script><p>Now since this holds for all <script type="math/tex">m^{'} \in V</script>, we can take the sum across all posible value of <script type="math/tex">m^{'}</script>:</p><script type="math/tex; mode=display">\left|V\right| > \lambda \left( \sum_{m^{'}, n^{'} \in V} t_{m^{'}} t_{n^{'}} \phi_{n^{'}}^T \phi_{m^{'}} \right) + \lambda B \sum_{m^{'} \in V} t_{m^{'}}</script><p><script type="math/tex">\lambda B\sum_{m^{'}} t_{m^{'}}</script> reduces to zero thanks to the second condition. Denote <script type="math/tex">t</script> as the vector whose entries are all <script type="math/tex">t_{m^{'} \in V}</script> and <script type="math/tex">\mathbb{K}</script> the kernel for dot product <script type="math/tex">\phi^T\phi</script>. This inequality is vectorized to</p><script type="math/tex; mode=display">\left|V\right| > \lambda t^T\mathbb{K} t</script><p>Notice that each entry of <script type="math/tex">t</script> is either <script type="math/tex">1</script> or <script type="math/tex">-1</script>, and length of <script type="math/tex">t</script> is exactly the size of <script type="math/tex">V</script>, the final form of our violated constraint is thus:</p><script type="math/tex; mode=display">\lambda^{-1} > x^T\mathbb{K}x \ \ where \ \left\|x\right\| = 1</script><p>We know with such value of <script type="math/tex">x</script>, value of <script type="math/tex">x^T\mathbb{K}x</script> lies between <script type="math/tex">\lambda_{min}</script> and <script type="math/tex">\lambda_{max}</script> - the smallest and largest eigenvalue of positive definite <script type="math/tex">\mathbb{K}</script>. Since we are trying to prove this will not hold, the sure-fire way is to set <script type="math/tex">\lambda^{-1} = \lambda_{min}</script></p><p>&nbsp;</p><p></p><div class = "divider"></div><div class = "center-foot"><h1 id = "share"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://thtrieu.github.io/notes/engineering-the-last-layer" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="https://twitter.com/intent/tweet/?text=Linear Support Vector Machine on top of a deep net&url=https://thtrieu.github.io/notes/engineering-the-last-layer" target = "_blank"><img src="http://i.imgur.com/phnE5v0.png (twitter icon with padding)" alt="alt text" /></a>     <a href="https://plus.google.com/share?url=https://thtrieu.github.io/notes/engineering-the-last-layer" target = "_blank"><img src="http://i.imgur.com/PeridnP.png (google plus icon with padding)" alt="alt text" /></a>     <a href="https://facebook.com/sharer/sharer.php?u=https://thtrieu.github.io/notes/engineering-the-last-layer" target = "_blank"><img src="http://i.imgur.com/GRvQu92.png (facebook icon with padding)" alt="alt text" /></a>     <a href="mailto:?subject=Linear Support Vector Machine on top of a deep net&body=https://thtrieu.github.io/notes/engineering-the-last-layer"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a></h1></div><aside class="back"> <a href="/notes/support-vector-machine-conversational-writeup">&laquo; Support Vector Machine - a conversational writeup</a></aside><aside class="next"> <a href="/notes/vectorizing-gradient-of-batch-normalization">Vectorizing gradient of high-dimensional Batch Normalization &raquo;</a></aside><p>&nbsp;</p><p></p><div id="disqus_thread"></div><script> /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://thtrieu.github.io/notes/engineering-the-last-layer'; this.page.identifier = 'https://thtrieu.github.io/notes/engineering-the-last-layer'; }; (function() { var d = document, s = d.createElement('script'); s.src = '//thtrieu.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); }) (); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article></main></body><script> var that = 'http://i.imgur.com/tU08Pu4.gif'; var thiS = 'http://i.imgur.com/tU08Pu4.gif'; var sub = 'http://i.imgur.com/w6NOJp2.gif'; var but = 'http://i.imgur.com/w6NOJp2.gif'; function postLogo() { document.getElementById("logo").src = 'http://i.imgur.com/tU08Pu4.gif'; } function thatLogo() { document.getElementById("logo").src = that; } function thisLogo() { document.getElementById("logo").src = thiS; } function subLogo() { document.getElementById("logo").src = sub; } function buttonLogo() { document.getElementById("logo").src = but; } function postClicked() { thiS = 'http://i.imgur.com/tU08Pu4.gif'; } function clicked() { thiS = that; } </script></html>