<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Support Vector Machine - a conversational writeup</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@" /><meta name="twitter:title" content="Support Vector Machine - a conversational writeup" /><meta name="twitter:description" content="This is a summary of Support Vector Machine, as for preparation of another post Linear SVM on top of a deep net"><meta name="description" content="This is a summary of Support Vector Machine, as for preparation of another post Linear SVM on top of a deep net"><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/support-vector-machine-conversational-writeup"><link rel="alternate" type="application/atom+xml" title="thtrieu" href="/feed.xml" /></head><body><aside class="logo"> <a href="/"> <img src="http://i.imgur.com/tU08Pu4.gif" id = "logo" onmouseover = "thatLogo()" onmouseout = "thisLogo()" onmouseup = "clicked()" class= "gravatar"> </a></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1 onmouseover="buttonLogo()" onmouseout = "thisLogo()"><a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#116;&#104;&#116;&#114;&#105;&#101;&#117;&#064;&#097;&#112;&#099;&#115;&#046;&#118;&#110;" target = "_blank"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a>     <a href="http://github.com/thtrieu/" target = "_blank"><img src="http://i.imgur.com/aV59QS6.png (github icon with padding)" alt="alt text" /></a>     <a href="https://linkedin.com/in/trinhhtrieu" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="http://thtrieu.github.io/resume.pdf" target = "_blank"><img src="http://i.imgur.com/2amdaUm.png (resume icon with padding)" alt="alt text" /></a>     <a href="#share"><img src="http://i.imgur.com/GE4GmC3.png (share icon with padding)" alt="alt text" /></a></h1><time>September 8, 2016 - just another day in heaven</time></div><!--<div class="divider"></div>--><p>&nbsp;</p><p></p><p>This is a summary of Support Vector Machine, as for preparation of another post <a href="https://thtrieu.github.io/notes/engineering-the-last-layer">Linear SVM on top of a deep net</a></p><h3 id="hard-margined-reasoning">Hard-margined reasoning</h3><p>Suppose we want to classify over data <script type="math/tex">(s_n, t_n)^N</script>, with <script type="math/tex">t_n \in \{-1, 1\}</script>. Define the kernel <script type="math/tex">\kappa(s_i, s_j) = \phi(s_i)^T\phi(s_j)</script> and the implicit decision boundary <script type="math/tex">y(s;\theta, b) = \theta^T\phi (s) + b</script> (implicit, since <script type="math/tex">\phi</script> is not explicitly defined). The euclidean distance from point <script type="math/tex">s_n</script> to the decision boundary is then:</p><script type="math/tex; mode=display">d_n = t_n \frac{y(s_n; \theta, b)}{\left \| \theta \right \|}</script><p>Suppose data is linearly separable, i.e. there is at least one solution surface, SVM tries to find the best one. Amongst all data points there is always one that minimises <script type="math/tex">d_n</script>. Say this happens at <script type="math/tex">s_{n'}</script>, SVM formulation says that we want to maximize <script type="math/tex">d_{n'}</script> to obtain the best surface. Let’s say we did successfully maximize the thing by <script type="math/tex">\theta^*</script>.</p><p>Since <script type="math/tex">\theta^*</script> is the normal vector of our solution surface, we can scale it without affecting the surface’s orientation (the solution). That means, if <script type="math/tex">\theta^*</script> is a solution, <script type="math/tex">c\theta^*</script> with <script type="math/tex">c</script> being a non-zero constant, is also one. Assume <script type="math/tex">t_ny(s_{n'}, \theta^*, b^*) = v</script>, we know there is some <script type="math/tex">c</script> that can be derived from <script type="math/tex">v</script> such that:</p><script type="math/tex; mode=display">t_n y(s_{n'}; c\theta^*, b^*) = 1</script><p>Now investigate <script type="math/tex">c\theta^*</script>. By definition of <script type="math/tex">n'</script>, this means <script type="math/tex">t_ny(s_n, c\theta^*, b^*) \geq 1 \ \ \forall n \ (*)</script>. Talking about the point <script type="math/tex">s_{n'}</script>, this means <script type="math/tex">c\theta^*</script> maximises</p><script type="math/tex; mode=display">d_{n'} = \frac{ 1 }{\left \| c\theta^* \right \| }</script><p>Equivalently, <script type="math/tex">c\theta^*</script> is also a maximizer of <script type="math/tex">{\left \| c\theta \right \|}^2 \ \ (**)</script></p><p>In both <script type="math/tex">(*)</script> and <script type="math/tex">(**)</script>, we realise that <script type="math/tex">c</script> and <script type="math/tex">\theta</script> is coupled and thus <script type="math/tex">c\theta</script> can be regarded as a single variable <script type="math/tex">w</script>. This allow a reformulation of the original optimization problem:</p><script type="math/tex; mode=display">\min_w \ \frac{1}{2} {\left \| w \right \|}^2</script> <center> subject to $$t_n y(s_n; w, b) \geq 1 \ \ \forall n$$ </center><h3 id="soft-margined-reasoning">Soft-margined reasoning</h3><p>The above reasoning assumes separable data, due to this ideal situation we are able to establish a strict standard on all data points base on <script type="math/tex">s_{n'}</script>. Things need to relax when it comes to the linearly inseparable case. Namely we have to introduce a slack variable <script type="math/tex">\xi_n</script> for each of the data points, so that some of them can violate the standard: <script type="math/tex">t_n y(s_n; w, b) \geq 1</script>. The soft-margined SVM objective is:</p><script type="math/tex; mode=display">\min_{\xi, w} \ C \sum_{n=1}^{N}\xi_n + \frac{1}{2} {\left \| w \right \|}^2</script> <center> subject to $$\xi_n \geq 0 \ \forall n$$ $$t_n y(s_n; w, b) \geq 1 - \xi_n$$</center><p>Collectively denote the variables of interest <script type="math/tex">\{\xi, w, b\}</script> as <script type="math/tex">x</script>, we rewrite the problem declaration:</p><script type="math/tex; mode=display">\min_x \ f(x) \ \ s.t. \ \ g_{1n}(x) \leq 0 \ \ and \ \ g_{2n}(x) \leq 0 \ \forall n</script> <center> where $$ f(x) = C \sum_{n=1}^{N}\xi_n + \frac{1}{2} {\left \| w \right \|}^2$$ $$g_{1n}(x) = -\xi_n$$ $$g_{2n}(x) = 1 - \xi_n - t_ny(s_n; w, b)$$ </center><p>Notice <script type="math/tex">x</script> is a collection of parameters with component <script type="math/tex">\xi</script>, whose number of parameters equals to the number of data training points.</p><h3 id="primal-and-dual-problems">Primal and Dual problems</h3><p>Is there an unconstrained version of the above problem, cuz constraints are kinda ugly isn’t it? In other words, is there a function <script type="math/tex">P(x)</script> such that minimizing <script type="math/tex">P(x)</script> yields the solution to our problem? In fact, there is one:</p><script type="math/tex; mode=display">P(x) = f(x) + \max_{\mu \geq 0, a \geq 0} \sum_n^N\mu_ng_{1n}(x) + a_ng_{2n}(x)</script><p>Equivalence can be mentally checked easily by investigating two cases. First, for each <script type="math/tex">x</script> that violates <strong>at least one</strong> constraint, check that <script type="math/tex">P(x)</script> reaches <script type="math/tex">+\infty</script> and thus cannot be a minimum. Second, for each <script type="math/tex">x</script> that satisfies <strong>all</strong> constraints, check that <script type="math/tex">P(x) \equiv f(x)</script>. Minimizing <script type="math/tex">P(x)</script> is called the <strong>Primal</strong> problem (which is essentially a <strong>minimax</strong> problem), and turns out this one is no less nasty than the original one since minimax problems are definitely not easy to handle.</p><p>Fortunately the whole Primal idea is not just about throwing away constraints, a guy named <a href="https://en.wikipedia.org/wiki/Slater%27s_condition">Slater</a> and his friend told us that, for the specific problem we are solving, the equivalence extends a bit further: the corresponding <strong>maximin</strong> problem yields the same solution to our minimax problem (Primal problem) and thus the same solution to our original problem. This maximin problem is called the <strong>Dual</strong> problem, and turns out to be quite nice to its pet human whenever they do some trick to its amuse. Next part will introduce that trick called KKT necessary conditions.</p><p>To have a clearer notation that faithfully serves the names “minimax” and “maximin”, let’s introduce <script type="math/tex">L(x, \mu, a) = f(x) + \sum_n^N\mu_ng_{1n}(x) + a_ng_{2n}(x)</script>. Then the Primal and Dual problems respectively become</p><script type="math/tex; mode=display">\min_x \max_{\mu \geq 0, a \geq 0} L(x, \mu, a)</script> <script type="math/tex; mode=display">\max_{\mu \geq 0, a \geq 0} \min_x L(x, \mu, a)</script><p>Absolutely on-point, isn’t it?</p><h3 id="solving-the-dual-problem-by-the-help-of-kkt-conditions">Solving the dual problem by the help of KKT conditions</h3><p>There is another giant’s shoulder to step on besides Slater’s in the quest for <script type="math/tex">x^*</script>. It concerns something called <strong>Karush-Kuhn-Tucker (KKT) conditions</strong>: conditions that are <em>necessary</em> for any solution <script type="math/tex">x^*</script> of our optimization problem. In other words, from the fact that <script type="math/tex">x^*</script> is a solution to our problem, what can we infer? Just for the record, these conditions later greatly simplify the Dual problem.</p><p>We can actually derive such conditions intuitively. Check <a href="http://www.onmyphd.com/?p=kkt.karush.kuhn.tucker">here</a> for a longer talk, I’ll just cherry pick here the sweetest parts for the sake of our SVM. First, look at an illustrative plot of <script type="math/tex">f(x)</script> and <script type="math/tex">g(x)</script>. There are only two possible scenarios:</p><center> <img src="http://www.onmyphd.com/pages/kkt/graphical.explanation.svg" alt="img1" style="width: 400px;" /> </center><p>On the left: the solution <script type="math/tex">\hat{x}</script> of the unconstrained problem satifies <script type="math/tex">g</script>, thus it is also solution <script type="math/tex">x^*</script> to the whole problem. To obtain <script type="math/tex">x^*</script>, we can optimize <script type="math/tex">f</script> regardless of <script type="math/tex">g</script>, i.e. minimizing <script type="math/tex">L(x)</script> with <script type="math/tex">\mu_n = a_n = 0 \ \forall n</script> solves an equivalent problem. On the right: solution to the unconstrained problem does not satify <script type="math/tex">g</script>, thus <script type="math/tex">x^*</script> lies on the boundary of <script type="math/tex">g</script>-satified region, where <script type="math/tex">g(x^*) = 0</script>. Again, minimizing <script type="math/tex">L(x)</script> along <script type="math/tex">g(x) = 0</script> solves an equivalent problem.</p><p>In fact, these two cases are analogous to the two checks we did with the Primal problem. They will be visited again in later parts. Either case, <script type="math/tex">\mu_ng_{1n}(x^*) = a_ng_{2n}(x^*) = 0</script>. I.e. our first two KKT conditions are:</p><script type="math/tex; mode=display">\mu_n \xi_n^* = 0 \ \forall n \ \ (1)</script> <script type="math/tex; mode=display">a_n(t_ny(s_n; w^*, b^*)-1+\xi_n^*) = 0 \ \forall n \ \ (2)</script><p>Remember that <script type="math/tex">x^*</script> solves the Dual problem <script type="math/tex">\{ \max_{\mu \geq 0, a \geq 0} \min_x L(x, \mu, a) \}</script>, which means <script type="math/tex">x^*</script> minimizes <script type="math/tex">L(x, \mu, a)</script>. This gives the next conditions: <script type="math/tex">\bigtriangledown_xL(x^*, \mu, a) = 0</script>. Taking the partial derivative with respect to <script type="math/tex">w</script>, <script type="math/tex">b</script>, and <script type="math/tex">\xi</script> separately yields:</p><script type="math/tex; mode=display">w^* = \sum_n^N a_nt_n\phi(s_n) \ \ (3)</script> <script type="math/tex; mode=display">\sum_n^N a_nt_n = 0 \ \ (4)</script> <script type="math/tex; mode=display">a_n = C - \mu_n \ \ (5)</script><p>The remaining conditions are obvious: <script type="math/tex">a_n \geq 0, \</script> <script type="math/tex">\mu_n \geq 0 \ \ (6)</script></p><h3 id="final-form-of-the-dual-problem">Final form of the Dual problem</h3><p>Now that’s everything is settled, we are ready to simplify the Dual problem as promised. In fact, we will use all KKT conditions that involves <script type="math/tex">x^*</script> as substitutions into the Dual objective. The obvious effect is that we completely eliminate <script type="math/tex">\{w, b, \xi \}</script> variables out of the Dual objective. Also use <script type="math/tex">(5)</script> to throw away <script type="math/tex">\mu</script>. Finally we obtain the simplified objective of maximization: <script type="math/tex">\tilde{L}(a) = \sum_n^N a_n - \frac{1}{2}\sum_i^N\sum_j^N a_ia_j\kappa(s_i, s_j)</script>. This optimization is subjected to <script type="math/tex">(4)</script>, <script type="math/tex">(5)</script> and <script type="math/tex">(6)</script> - conditions that involves <script type="math/tex">a</script> - which, can be rewritten as</p><script type="math/tex; mode=display">a_i \in [ 0, C ]</script> <script type="math/tex; mode=display">\sum_n^N a_nt_n = 0</script><p>But there is something else happening, a side effect that reveals itself after the derivation of <script type="math/tex">\tilde{L}</script>: <script type="math/tex">\kappa</script>, who emerges from the disappearance of <script type="math/tex">\phi</script>. They say the thing did its <strong>kernel trick</strong>. But what’s good about <script type="math/tex">\kappa</script> anyway? To shortly explain the hype around <script type="math/tex">\kappa</script>, consider solving the Dual problem in this final form. Clearly we only have to deal with <script type="math/tex">\kappa</script>, not <script type="math/tex">\phi</script> explicitly. There’s a body of theories around this situation that states that there are certain <script type="math/tex">\kappa</script> which are extremely convenient to deal with mathematically, who turn out to have their <script type="math/tex">\phi</script> ultra-high-dimensional, sometimes even infinite.</p><p>In other words, we are able to learn non-parameterized high-quality features without the need to deal with the curse of dimensionality. In the context of <strong>Deep Learning</strong>, <script type="math/tex">\phi</script> is <strong>explicitly defined</strong> as the feature extractor of the network and its parameters are trained jointly with <script type="math/tex">x^*</script>. We’ll come back to this point in Part 2.</p><h3 id="sequential-minimal-optimization-the-dual-solver">Sequential minimal optimization: the Dual solver</h3><p>How come this final form of Dual objective nicer to solve than, say, the Original and the Primal?</p><p>Realise that the Dual objective is a polynomial of degree 2 for each of the <script type="math/tex">a_n</script>, the problem is thus convex and if we are able to progressively better the objective until convergence, the solution is found. So, we are talking about an interative algorithm and wish to define a convergence criteria for it. In fact, the KKT conditions <script type="math/tex">(1)</script> and <script type="math/tex">(2)</script> implies three things that can be used to check convergence around the optimal point:</p><script type="math/tex; mode=display">1. \ \ a_n = 0 \Rightarrow t_ny(s_n; w^*, b^*) \geq 1</script> <script type="math/tex; mode=display">2. \ \ a_n \in (0, C) \Rightarrow t_ny(s_n; w^*, b^*) = 1</script> <script type="math/tex; mode=display">3. \ \ a_n = C \Rightarrow t_ny(s_n; w^*, b^*) \leq 1</script><p>There is this guy <a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization">John Platt</a> who proposed the algorithm we are looking for. His reasoning goes roughly like this: pick a subset from <script type="math/tex">\{a_1, a_2, ..., a_N \}</script> and optimize the objective with respect to this subset, because that’s easier than dealing with the whole bunch. Pick another one and another one and repeat until convergence. Basically each time we optimize a sub-problem, and this problem has to have at least two variables because otherwise, the equality constraint <script type="math/tex">(4)</script> gives no room for (single-variable) optimization. And in fact he did choose <script type="math/tex">2</script> to be the size of all sub-problems.</p><p>The sub-problem goes as follows: Maximize <script type="math/tex">\tilde{L}</script> with respect to a pair of variable <script type="math/tex">\{a_n, a_m\}</script> such that <script type="math/tex">0 \leq a_n, a_m \leq C</script>. From <script type="math/tex">(4)</script> we get <script type="math/tex">t_na_n + t_ma_m = k</script> where <script type="math/tex">k</script> is the current value of <script type="math/tex">- \sum_{i \not = n, m} t_ia_i</script>. This gives a one to one correspondence between <script type="math/tex">a_n</script> and <script type="math/tex">a_m</script>, hence the sub-problem reduce to a single-variable optimization problem. Say we choose to express <script type="math/tex">a_n</script> in terms of <script type="math/tex">a_m</script> and solve the single-variable problem in <script type="math/tex">a_m</script>, there is another thing to care about: the bound <script type="math/tex">[0, C]</script> of <script type="math/tex">a_m</script> must be adjusted so that <script type="math/tex">a_n</script> also lies in <script type="math/tex">[0, C]</script>.</p><p>Optimizing such single-variable quadratic function is something quite trivial: maximizer of <script type="math/tex">q(x) = -x + c_1x + c2</script> is <script type="math/tex">c_1/2</script>. And since the problem is convex, the constraints can be handled easily by clipping the solution of the non-constrained problem back the the boundary of the feasible region. All the nitty-gritties that ensure convergence and heuristics to maximise algorithm’s speed is discussed in the <a href="https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/">original paper</a>. And there’s that for SVM optimization!</p><p>&nbsp;</p><p></p><div class = "divider"></div><div class = "center-foot"><h1 id = "share"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="https://twitter.com/intent/tweet/?text=Support Vector Machine - a conversational writeup&url=https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup" target = "_blank"><img src="http://i.imgur.com/phnE5v0.png (twitter icon with padding)" alt="alt text" /></a>     <a href="https://plus.google.com/share?url=https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup" target = "_blank"><img src="http://i.imgur.com/PeridnP.png (google plus icon with padding)" alt="alt text" /></a>     <a href="https://facebook.com/sharer/sharer.php?u=https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup" target = "_blank"><img src="http://i.imgur.com/GRvQu92.png (facebook icon with padding)" alt="alt text" /></a>     <a href="mailto:?subject=Support Vector Machine - a conversational writeup&body=https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a></h1></div><aside class="back"> <a href="/notes/Invariances-and-people-of-the-circle">&laquo; Invariances of N people (with balls) running around in circle</a></aside><aside class="next"> <a href="/notes/engineering-the-last-layer">Linear Support Vector Machine on top of a deep net &raquo;</a></aside><p>&nbsp;</p><p></p><div id="disqus_thread"></div><script> /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup'; this.page.identifier = 'https://thtrieu.github.io/notes/support-vector-machine-conversational-writeup'; }; (function() { var d = document, s = d.createElement('script'); s.src = '//thtrieu.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); }) (); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article></main></body><script> var that = 'http://i.imgur.com/tU08Pu4.gif'; var thiS = 'http://i.imgur.com/tU08Pu4.gif'; var sub = 'http://i.imgur.com/w6NOJp2.gif'; var but = 'http://i.imgur.com/w6NOJp2.gif'; function postLogo() { document.getElementById("logo").src = 'http://i.imgur.com/tU08Pu4.gif'; } function thatLogo() { document.getElementById("logo").src = that; } function thisLogo() { document.getElementById("logo").src = thiS; } function subLogo() { document.getElementById("logo").src = sub; } function buttonLogo() { document.getElementById("logo").src = but; } function postClicked() { thiS = 'http://i.imgur.com/tU08Pu4.gif'; } function clicked() { thiS = that; } </script></html>