<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /><title>Vectorizing gradient of high-dimensional Batch Normalization</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@" /><meta name="twitter:title" content="Vectorizing gradient of high-dimensional Batch Normalization" /><meta name="twitter:description" content=""><meta name="description" content=""><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/vectorizing-gradient-of-batch-normalization"><link rel="alternate" type="application/atom+xml" title="thtrieu" href="/feed.xml" /></head><body><aside class="logo"> <a href="/"> <img src="http://i.imgur.com/tU08Pu4.gif" id = "logo" onmouseover = "thatLogo()" onmouseout = "thisLogo()" onmouseup = "clicked()" class= "gravatar"> </a></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><article><div class="center"><h1 onmouseover="buttonLogo()" onmouseout = "thisLogo()"><a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#116;&#104;&#116;&#114;&#105;&#101;&#117;&#064;&#097;&#112;&#099;&#115;&#046;&#118;&#110;" target = "_blank"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a>     <a href="http://github.com/thtrieu/" target = "_blank"><img src="http://i.imgur.com/aV59QS6.png (github icon with padding)" alt="alt text" /></a>     <a href="https://linkedin.com/in/trinhhtrieu" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="http://thtrieu.github.io/resume.pdf" target = "_blank"><img src="http://i.imgur.com/2amdaUm.png (resume icon with padding)" alt="alt text" /></a>     <a href="#share"><img src="http://i.imgur.com/GE4GmC3.png (share icon with padding)" alt="alt text" /></a></h1><time>December 27, 2016 - just another day in heaven</time></div><!--<div class="divider"></div>--><p>&nbsp;</p><p></p><ul id="markdown-toc"><li><a href="#batch-normalization-overview" id="markdown-toc-batch-normalization-overview">Batch normalization overview</a><ul><li><a href="#goal" id="markdown-toc-goal">Goal</a></li><li><a href="#denotations" id="markdown-toc-denotations">Denotations</a></li></ul></li><li><a href="#a-computational-graph-view-of-batch-normalization" id="markdown-toc-a-computational-graph-view-of-batch-normalization">A computational graph view of Batch Normalization</a></li><li><a href="#piece-the-pieces-together-then-simplify" id="markdown-toc-piece-the-pieces-together-then-simplify">Piece the pieces together, then simplify</a></li><li><a href="#the-code" id="markdown-toc-the-code">The code</a><ul><li><a href="#forward-pass" id="markdown-toc-forward-pass">Forward pass</a></li><li><a href="#backward-pass" id="markdown-toc-backward-pass">Backward pass</a></li></ul></li></ul><h3 id="batch-normalization-overview">Batch normalization overview</h3><p>Batch normalization is a technique that stablizes the input distribution of intermediate layers in deepnet during training, this reduces the effect of information morphing and thus helps speed up training (during a few first steps). Batch-norm is claimed to reduce the need of using dropout due to its regularizing effect. As the name <code class="highlighter-rouge">Normalization</code> suggests, you simply normalize the output of the layer to zero mean and unit variance:</p><script type="math/tex; mode=display">\hat{x} = Norm(x, \mathbb{X})</script><p>This requires expensive computation on <script type="math/tex">Cov[x]</script> and its inverse square root <script type="math/tex">Cov[x]^{-1/2}</script>, so an approximation over each mini-batch during training is proposed. For a layer with input <script type="math/tex">\mathbb{B} = \{x_{1..m}\}</script>, besides its original parameters to learn, <code class="highlighter-rouge">Batch normalization</code> introduces two other learnable parameters: <script type="math/tex">\gamma</script> and <script type="math/tex">\beta</script>. The forwarding proceeds as follows</p><script type="math/tex; mode=display">\mu_{\mathbb{B}} \leftarrow \frac{1}{m}\sum_{i=1}^mx_i</script> <script type="math/tex; mode=display">\sigma_{\mathbb{B}} \leftarrow \frac{1}{m}\sum_{i=1}^m(x_i - \mu_{\mathbb{B}})^2</script> <script type="math/tex; mode=display">\hat{x_i} \leftarrow \frac{x_i - \mu_{\mathbb{B}}}{\sqrt{\sigma^2_{\mathbb{B}} + \epsilon}}</script> <script type="math/tex; mode=display">y_i \leftarrow \gamma \hat{x_i} + \beta \equiv BN_{\gamma, \beta}(x_i)</script><h4 id="goal">Goal</h4><p>The above equation describes how BN handles scalars input. Real implementation, however, deals with much higher dimension vectors/ matrices. As an example, an information volume flowing through convolutional layer is 4 dimensional and we only takes the normalization steps over each feature map, separately.</p><p>Let the volume of interest be <script type="math/tex">k</script> dimensional and we are normalizing over the first <script type="math/tex">k-1</script> dimensions, I will derive a sequence of vectorized operations on <script type="math/tex">\triangledown_yLoss</script> - the gradient propagated back to this layer - to produce <script type="math/tex">\triangledown_xLoss</script> and <script type="math/tex">\triangledown_{\gamma}Loss</script>. <script type="math/tex">\triangledown_{\beta}Loss</script> will not be considered as it can be cast as a bias-adding operation and does not belong to the <em>atomic</em> view of batch-normalization.</p><h4 id="denotations">Denotations</h4><p>For ease of denotation, I denote <script type="math/tex">\triangledown_xLoss</script> as <script type="math/tex">\delta x</script> and consider the case of one dimensional vector: <script type="math/tex">x_{ij}</script> being the jth feature of ith training example in the batch. The design matrix is then <script type="math/tex">x</script> of size <script type="math/tex">n \times f</script> where <script type="math/tex">n</script> is n is the batch size and <script type="math/tex">f</script> is number of features. Although we are limiting the analysis to only one dimensional vectors, the later code is applicable to arbitralily bigger number of dimensions.</p><p>For example, a 4 dimensional volume with shape <script type="math/tex">(n, h, w, f)</script> can be considered as <script type="math/tex">(n*h*w, f)</script> after reshaping. For numpy broadcasting ability, reshaping is not even necessary.</p><h3 id="a-computational-graph-view-of-batch-normalization">A computational graph view of Batch Normalization</h3><p>First we break the process into simpler parts</p><center> <img src="https://i.imgur.com/ZSiV1tf.png" /> </center><p>Namely,</p><script type="math/tex; mode=display">m = \frac{1}{n}\sum_{i = 1}^nx_i</script> <script type="math/tex; mode=display">\overline{x_i} = x_i - m</script> <script type="math/tex; mode=display">v = \frac{1}{n}\sum_{i=1}^n\overline{x_i}^2</script> <script type="math/tex; mode=display">x^*_{ij} = \frac{\overline{x_{ij}}}{\sqrt{v_j}}</script> <script type="math/tex; mode=display">x^{BN} = \gamma x^*</script><p>In our setting <script type="math/tex">\delta x^{BN}</script> is available. Gradients flows backwards, so we first consider <script type="math/tex">\delta x^*</script>. Each entry <script type="math/tex">x^*_{ij}</script> contribute to the loss only through <script type="math/tex">x^{BN}_{ij}</script>, so according to chain rule:</p><script type="math/tex; mode=display">\delta x^*_{ij} = \delta x^{BN}_{ij} * \partial x^{BN}_{ij} / \partial x^*_{ij} = \gamma \delta x^{BN}_{ij}</script> <script type="math/tex; mode=display">\Rightarrow \delta x^* = \gamma x^{BN}</script><p>Next, we can do either <script type="math/tex">v</script> or <script type="math/tex">\overline{x}</script>, <script type="math/tex">v</script> is simpler since it contributes to the loss only through <script type="math/tex">x^*</script> as shown in the graph (while <script type="math/tex">\overline{x}</script> also contributes to the loss through <script type="math/tex">v</script>). Consider a single entry <script type="math/tex">v_j</script>, it contributes to the loss through <script type="math/tex">x^*_{ij}</script> for all <script type="math/tex">i</script>, so according to chain rule:</p><script type="math/tex; mode=display">\delta v_j = \sum_i \delta x^*_{ij} \frac{\partial x^*{ij}}{\partial v_j} = \sum_i \delta x^*_{ij} \frac{\partial (\overline{x}_{ij} v_j^{-1/2})}{ \partial v_j} = -1/2 v_j ^{-3/2} \sum_i \delta x^*_{ij} \overline{x}_{ij}</script> <script type="math/tex; mode=display">\Rightarrow \delta v_j = -1/2 v_j^{-3/2} \left( \sum_{i} x^*_i \odot \overline{x}_i \right)_j</script><p>Where <script type="math/tex">\odot</script> denotes elemenet-wise multiplication and the power of <script type="math/tex">-3/2</script> is applied element-wise. Move on to <script type="math/tex">x^2</script> with <script type="math/tex">v</script> being its mean, the gradient can be easily shown to be uniformly <em>spreaded</em> out from <script type="math/tex">v</script> as follows:</p><script type="math/tex; mode=display">\delta \overline{x}^2_i = \frac{1}{n} \delta v</script><p>We are now ready to calculate <script type="math/tex">\delta \overline{x}</script>, since it contributes to the loss through <script type="math/tex">x^*</script> and <script type="math/tex">x^2</script>, its gradient consists of two parts, one coming from <script type="math/tex">x^*</script> and the other from <script type="math/tex">x^2</script>. Let’s do the <script type="math/tex">x^2</script> first, since this square is applied element-wise, there is no summing in the derivative chain:</p><script type="math/tex; mode=display">\delta_{x^2} \overline{x}_{ij} = \delta \overline{x}^2_{ij} \partial \overline{x}^2_{ij} / \partial \overline{x}_{ij} = \delta \overline{x}^2_{ij} \partial \overline{x}_{ij}^2 / \partial \overline{x}_{ij} = 2\delta \overline{x}^2_{ij} \overline{x}_{ij}</script> <script type="math/tex; mode=display">\Rightarrow \delta_{x^2} \overline{x} = 2 \delta x^2 \odot \overline{x}</script><p>For <script type="math/tex">x^*</script>, <script type="math/tex">\overline{x}_{ij}</script> contributes to the loss through only <script type="math/tex">x^*_{ij}</script>, so there is also no summing in the chain:</p><script type="math/tex; mode=display">\delta_{x^*} \overline{x}_{ij} = \delta x^*_{ij} / v_j^{-1/2}</script><p>There is no matrix-wide equation this time, however if we extend the definition of <script type="math/tex">\odot</script> from element-wise to broadcasted mutiplication, then:</p><script type="math/tex; mode=display">\delta_{x^*} \overline{x} = {v}^{-1/2} \delta x^*</script><p>Take the sum of <script type="math/tex">\delta_{x^2}\overline{x}</script> and <script type="math/tex">\delta_{x^*}\overline{x}</script>, we have</p><script type="math/tex; mode=display">\delta \overline{x} = 2 \delta x^2 \odot \overline{x} + v^{-1/2}\delta x^*</script><p>Now for <script type="math/tex">m</script>, each entry in <script type="math/tex">m_j</script> contributes to the loss through the whole <script type="math/tex">jth</script> colume of <script type="math/tex">\overline{x}</script>, so:</p><script type="math/tex; mode=display">\delta m_j = \sum_i \delta \overline{x}_{ij} \partial \overline{x}_{ij} / \partial {m_j} = \sum_i \delta \overline{x}_{ij} \partial (x_{ij} - m_j) / \partial {m_j} = - \sum_i \delta \overline{x}_{ij}</script> <script type="math/tex; mode=display">\Rightarrow \delta m = - \sum_i \delta \overline{x}_i</script><p><script type="math/tex">x</script> contributes to the loss through <script type="math/tex">m</script> and <script type="math/tex">\overline{x}</script>, so its gradient is the sum of two parts. The part corresponds to <script type="math/tex">m</script> is analogous to that of <script type="math/tex">\overline{x}^2</script> and <script type="math/tex">v</script> in the sense that one is the row mean of the other. Therefore we can quickly derive that part to be <script type="math/tex">\delta_{m} x = \frac{1}{n} \delta{m}</script>.</p><p>The other part is also simple, as <script type="math/tex">\overline{x} = x - m</script>, there is no interaction between <script type="math/tex">x</script> and <script type="math/tex">m</script>, hence <script type="math/tex">\delta_{\overline{x}} x = \delta \overline{x}</script>. So finally <script type="math/tex">\delta x = \delta \overline{x} + \frac{1}{n} \delta m</script>.</p><h3 id="piece-the-pieces-together-then-simplify">Piece the pieces together, then simplify</h3><p>Remember that the goal is to derive <script type="math/tex">\delta x</script>, we’ll do it now using the results derived above:</p><script type="math/tex; mode=display">\delta x = \delta \overline{x} + \frac{1}{n} \delta m</script> <script type="math/tex; mode=display">= \delta \overline{x} -1/n \sum_i \delta \overline{x}</script><p>Interestingly this is the action of centering <script type="math/tex">\delta \overline{x}</script> around zeros, precisely what <script type="math/tex">\overline{x}</script> did to <script type="math/tex">x</script>.</p><script type="math/tex; mode=display">\delta \overline{x} = \delta {x^*} v^{-1/2} + 2\delta x^2 \odot \overline{x}</script> <script type="math/tex; mode=display">= \gamma v^{-1/2} \delta x^{BN} + \frac{2}{n}\delta v \odot \overline{x}</script> <script type="math/tex; mode=display">= \gamma v^{-1/2} \delta x^{BN} - \frac{1}{n} v^{-3/2} \odot \sum_i \left (\delta x^* \odot \overline{x}\right )_i \odot \overline{x}</script> <script type="math/tex; mode=display">= \gamma v^{-1/2} \delta x^{BN} - \frac{\gamma}{n} v^{-3/2} \odot \sum_i \left (\delta x^{BN} \odot \overline{x}\right )_i \odot \overline{x}</script> <script type="math/tex; mode=display">= \gamma v^{-1/2} \left ( \delta x^{BN} - \frac{1}{n} \sum_i \left (\delta x^{BN} \odot \overline{x}v^{-1/2}\right )_i \odot \overline{x}v^{-1/2}\right )</script> <script type="math/tex; mode=display">= \gamma v^{-1/2} \left ( \delta x^{BN} - \frac{1}{n} \sum_i \left (\delta x^{BN} \odot x^*\right )_i \odot x^*\right )</script><p>We are done here. For efficient computation, in the forward pass we will save the value of <script type="math/tex">v^{-1/2}</script> and <script type="math/tex">x^*</script>. This will not add anything to the computation complexity of forward pass.</p><h3 id="the-code">The code</h3><h4 id="forward-pass">Forward pass</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">is_training</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fd</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fd</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mv_mean</span><span class="o">.</span><span class="n">apply_update</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mv_var</span><span class="o">.</span><span class="n">apply_update</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mv_mean</span><span class="o">.</span><span class="n">val</span>
        <span class="n">var</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mv_var</span><span class="o">.</span><span class="n">val</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_rstd</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_normed</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rstd</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normed</span> <span class="o">*</span> <span class="n">gamma</span>
</code></pre></div></div><h4 id="backward-pass">Backward pass</h4><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">g_gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normed</span><span class="p">)</span>
    <span class="n">g_gamma</span> <span class="o">=</span> <span class="n">g_gamma</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fd</span><span class="p">)</span>
    <span class="n">x_</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normed</span> <span class="o">*</span> <span class="n">g_gamma</span> <span class="o">*</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">N</span>
    <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rstd</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gamma</span> <span class="o">*</span> <span class="n">x_</span>
    <span class="k">return</span> <span class="n">x_</span> <span class="o">-</span> <span class="n">x_</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_fd</span><span class="p">),</span> <span class="n">g_gamma</span>
</code></pre></div></div><p>The code is taken from a Github repo of mine where I am building something similar to an audo-diff DAG graph. <a href="https://github.com/thtrieu/numpyflow">Visit</a> if you are interested. I conclude the post here.</p><p>&nbsp;</p><p></p><div class = "divider"></div><div class = "center-foot"><h1 id = "share"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization" target = "_blank"><img src="http://i.imgur.com/Q9Dr6XJ.png (linkedin icon with padding)" alt="alt text" /></a>     <a href="https://twitter.com/intent/tweet/?text=Vectorizing gradient of high-dimensional Batch Normalization&url=https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization" target = "_blank"><img src="http://i.imgur.com/phnE5v0.png (twitter icon with padding)" alt="alt text" /></a>     <a href="https://plus.google.com/share?url=https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization" target = "_blank"><img src="http://i.imgur.com/PeridnP.png (google plus icon with padding)" alt="alt text" /></a>     <a href="https://facebook.com/sharer/sharer.php?u=https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization" target = "_blank"><img src="http://i.imgur.com/GRvQu92.png (facebook icon with padding)" alt="alt text" /></a>     <a href="mailto:?subject=Vectorizing gradient of high-dimensional Batch Normalization&body=https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization"><img src="http://i.imgur.com/CKLpgcs.png (email icon with padding)" alt="alt text" /></a></h1></div><aside class="back"> <a href="/notes/engineering-the-last-layer">&laquo; Linear Support Vector Machine on top of a deep net</a></aside><aside class="next"> <a href="/notes/Sakura-tree">Sakura blossoms by the silent construction site &raquo;</a></aside><p>&nbsp;</p><p></p><div id="disqus_thread"></div><script> /** * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS. * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */ var disqus_config = function () { this.page.url = 'https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization'; this.page.identifier = 'https://thtrieu.github.io/notes/vectorizing-gradient-of-batch-normalization'; }; (function() { var d = document, s = d.createElement('script'); s.src = '//thtrieu.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); }) (); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></article></main></body><script> var that = 'http://i.imgur.com/tU08Pu4.gif'; var thiS = 'http://i.imgur.com/tU08Pu4.gif'; var sub = 'http://i.imgur.com/w6NOJp2.gif'; var but = 'http://i.imgur.com/w6NOJp2.gif'; function postLogo() { document.getElementById("logo").src = 'http://i.imgur.com/tU08Pu4.gif'; } function thatLogo() { document.getElementById("logo").src = that; } function thisLogo() { document.getElementById("logo").src = thiS; } function subLogo() { document.getElementById("logo").src = sub; } function buttonLogo() { document.getElementById("logo").src = but; } function postClicked() { thiS = 'http://i.imgur.com/tU08Pu4.gif'; } function clicked() { thiS = that; } </script></html>